# ClearPath Chatbot - API Contract Specification

---

## Required Endpoint

### POST /query

**Description**: Accepts a user question and returns the chatbot's response along with metadata about the retrieval and generation process.

**Request Format**:
```json
{
  "question": "What is the price of the Pro plan?",
  "conversation_id": "optional-conversation-id-for-multi-turn"
}
```

**Response Format**:
```json
{
  "answer": "The Pro plan pricing appears in the documentation with conflicting information...",
  "metadata": {
    "model_used": "llama-3.3-70b-versatile",
    "classification": "complex",
    "tokens": {
      "input": 1234,
      "output": 156
    },
    "latency_ms": 847,
    "chunks_retrieved": 3,
    "evaluator_flags": []
  },
  "sources": [
    {
      "document": "14_Pricing_Sheet_2024.pdf",
      "page": 1,
      "relevance_score": 0.92
    },
    {
      "document": "15_Enterprise_Plan_Details.pdf", 
      "page": 1,
      "relevance_score": 0.88
    }
  ],
  "conversation_id": "conv_abc123"
}
```

---

## Field Specifications

### Request Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `question` | string | Yes | The user's query |
| `conversation_id` | string | No | For maintaining conversation context across multiple turns |

### Response Fields

#### Top-Level Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `answer` | string | Yes | The chatbot's natural language response |
| `metadata` | object | Yes | Information about the query processing |
| `sources` | array | Yes | Documents used to generate the answer (can be empty array) |
| `conversation_id` | string | Yes | Conversation identifier (generate if not provided) |

#### metadata Object

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `model_used` | string | Yes | Which LLM was used: "llama-3.1-8b-instant" or "llama-3.3-70b-versatile" |
| `classification` | string | Yes | Router classification: "simple" or "complex" |
| `tokens` | object | Yes | Token usage breakdown |
| `tokens.input` | integer | Yes | Input tokens sent to LLM |
| `tokens.output` | integer | Yes | Output tokens generated by LLM |
| `latency_ms` | integer | Yes | Total time from request to response in milliseconds |
| `chunks_retrieved` | integer | Yes | Number of document chunks retrieved by RAG |
| `evaluator_flags` | array | Yes | List of flags raised by your evaluator (e.g., ["no_context", "low_confidence"]) - empty array if none |

#### sources Array

Each source object:

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `document` | string | Yes | Document filename (e.g., "14_Pricing_Sheet_2024.pdf") |
| `page` | integer | No | Page number if applicable |
| `relevance_score` | float | No | Similarity/relevance score (0-1) |

**Note**: If no relevant documents were retrieved, return an empty array `[]`

---

## Routing Classification Rules

Your router must classify queries as `"simple"` or `"complex"` using deterministic, rule-based logic (NOT an LLM call).

### Required Behavior:
- Route `"simple"` queries → `llama-3.1-8b-instant`
- Route `"complex"` queries → `llama-3.3-70b-versatile`

### You Define the Rules
The exact classification rules are up to you, but they must be:
1. **Deterministic** - same query always gets same classification
2. **Rule-based** - explicit logic (e.g., query length, keyword presence, question marks)
3. **Documented** - you'll explain your rules in Q1 of written answers

Example rule systems (pick one or design your own):
- **Length-based**: <10 words = simple, ≥10 words = complex
- **Keyword-based**: Contains ["how", "why", "explain", "compare"] = complex
- **Heuristic**: Simple if (length < 8 words AND no subordinate clauses AND ≤1 question mark)

---

## Evaluator Flags

Your output evaluator must check responses and flag issues. Return flag strings in `metadata.evaluator_flags`:

### Required Flags (Minimum 3):

1. **`"no_context"`** - LLM answered but no relevant chunks were retrieved
   - Condition: `chunks_retrieved == 0` but answer is not a refusal

2. **`"refusal"`** - LLM explicitly refused to answer or said it doesn't know
   - Condition: Answer contains phrases like "I don't have", "not mentioned", "cannot find"

3. **`"<your_custom_flag>"`** - You define one domain-specific check
   - Examples: `"pricing_uncertainty"`, `"outdated_source"`, `"multiple_conflicting_sources"`
   - Explain what it is and why you chose it in Q2

### Flag Behavior:
- Flags are **informational** - still return the answer
- Multiple flags can be raised for one response
- Empty array `[]` if no flags

---

## Deployment Requirements

Your API must be accessible for evaluation. Choose one:

1. **Local deployment** - Must provide clear instructions to run on `localhost:8000`
2. **Public deployment** - Provide a publicly accessible URL (Vercel, Railway, Render, etc.)
   - **Bonus points** for AWS/GCP deployment

### Environment Variables

Document any required environment variables in your README:
```bash
GROQ_API_KEY=your_groq_api_key
PORT=8000  # Optional, defaults to 8000
```
